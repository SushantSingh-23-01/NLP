{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Between 1 dimensional matrices or for single token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let each token be represented by 'E' embedding vectors (e.g. 5 in case below), we calculate cosine simialrity and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2529, -1.2352, -0.1684,  0.2056, -0.0381]) tensor([-0.2762,  0.0028,  0.2351, -1.5043, -0.0251])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(1,5).flatten()\n",
    "x2 = torch.randn(1,5).flatten()\n",
    "print(x1,x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Caution`: Keep check of tensor dimensions, randn gives 2-d tensor, so use flatten() to get 1-d tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4213)\n"
     ]
    }
   ],
   "source": [
    "x1x2 = torch.dot(x1.squeeze(),x2.squeeze())\n",
    "print(x1x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9951)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den = torch.norm(x1,dim=0)*torch.norm(x2,dim=0)\n",
    "den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2112)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = x1x2 / den\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21117093"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.dot(x1.numpy(),x2.numpy()) / ( np.linalg.norm(x1.numpy()) * np.linalg.norm(x2.numpy()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using direct implementation in Pytorch<br>\n",
    "<br>https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html <br>\n",
    "- `dim` (int, optional) – Dimension where cosine similarity is computed. Default: 1.\n",
    "- `eps` (float, optional) – Small value to avoid division by zero. Default: 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2112)\n"
     ]
    }
   ],
   "source": [
    "output = cos(x1,x2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Observation 1`: for 1 dimesional matrix (that is one token/word for word level embeddings) we will get one score value irrespective of embedding size.<br>\n",
    "*shape(cos_sim) for 1xE and 1xE matrices is 1x1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between multiple tokens embed vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider multiple tokens (3 in case below). Now our size increases to 3x5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1605, -0.1976,  0.7486, -0.9748, -0.3268],\n",
      "        [ 1.2063,  1.5380,  0.4492,  0.3619, -0.6589],\n",
      "        [ 0.0809,  0.4931, -0.1440,  1.7738,  1.2533]])\n",
      "tensor([[ 0.3088, -0.4771, -1.1066,  1.1779, -1.1083],\n",
      "        [ 0.6225, -0.1443,  0.0154,  1.9311,  0.1449],\n",
      "        [ 1.9074,  0.5856,  0.9995, -1.0327, -0.3269]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(3,5)\n",
    "x2 = torch.randn(3,5)\n",
    "print(f'{x1}\\n{x2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity : tensor([-0.5556,  0.2608, -0.3506])\n"
     ]
    }
   ],
   "source": [
    "output = cos(x1,x2)\n",
    "print(f'Cosine Similarity : {output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its always good idea when learning to cross check manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5556,  0.2608, -0.3506])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = []\n",
    "for i in range(x1.size(0)):\n",
    "    x1x2 = torch.dot(x1[i],x2[i])\n",
    "    den = torch.norm(x1[i],dim=0)*torch.norm(x2[i],dim=0)\n",
    "    score.append(x1x2/den)\n",
    "score = torch.stack(score)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Observation` :  For 3 tokens we get 3 scores which is expected.\n",
    "<br> *But there seems to be few problems :*\n",
    "- We are calculating similarity order wise or along indices of sequences. This leads to problem that sentences with words having similar meaning at different positions in sentences will result in lower similarities.\n",
    "-  We want a single similarity score at the end rather than matrix of similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than 3x1 similarity matrix we would like to calculate similarities between each word of both sequences. That is we would require a matrix of size 3x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.dot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1 <-> doc2\n",
      "tok0 <-> tok0 | Cosine Similarity : -0.555626\n",
      "tok0 <-> tok1 | Cosine Similarity : -0.676687\n",
      "tok0 <-> tok2 | Cosine Similarity : 0.637864\n",
      "tok1 <-> tok0 | Cosine Similarity : 0.068217\n",
      "tok1 <-> tok1 | Cosine Similarity : 0.260845\n",
      "tok1 <-> tok2 | Cosine Similarity : 0.657279\n",
      "tok2 <-> tok0 | Cosine Similarity : 0.142489\n",
      "tok2 <-> tok1 | Cosine Similarity : 0.786926\n",
      "tok2 <-> tok2 | Cosine Similarity : -0.350599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5556, -0.6767,  0.6379],\n",
       "        [ 0.0682,  0.2608,  0.6573],\n",
       "        [ 0.1425,  0.7869, -0.3506]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = []\n",
    "print('doc1 <-> doc2')\n",
    "for i in range(x1.size(0)):\n",
    "    for j in range(x1.size(0)):\n",
    "        score.append(cos(x1[i],x2[j]))\n",
    "        print(f'tok{i} <-> tok{j} | Cosine Similarity : {cos(x1[i],x2[j]):0.6f}')\n",
    "score = torch.stack(score).reshape(3,3)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Links: \n",
    "- https://www.baeldung.com/cs/sentence-vectors-word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 1 : Taking Mean of embeddings for each token in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9615)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(x1.mean(dim=1),x2.mean(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other approaches that are mentioned in the article, I would implement them with real text data to get better intuition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

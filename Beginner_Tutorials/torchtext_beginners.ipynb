{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginners Guide to Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field,get_tokenizer, BucketIterator,TabularDataset\n",
    "from torchtext.datasets import LanguageModelingDataset, WikiText2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext Documentation: https://torchtext.readthedocs.io/en/latest/ <br>\n",
    "Other tutorial Links:\n",
    "https://dzlab.github.io/dltips/en/pytorch/torchtext-datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting a Field object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of a Field object :**<br>\n",
    "Intuitively, it sets rules for preprocessing the input text, and creates a vocabulary of the words that are introduced from the data\n",
    "Things you can do with a Field object\n",
    "- 1.1. apply self-defined or external tokenizers to tokenize strings into word tokens\n",
    "- 1.2. automatically convert word tokens (strings) to indices (ints)\n",
    "- 1.3. automatically add SOS(start-of-sentence) or EOS(end-of-sentence) tokens to input strings\n",
    "- 1.4. convert text to lowercase\n",
    "- 1.5. determine whether to pad sentences to a fixed length or leave them as variable lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things you CAN'T do in a Field object :**<br>\n",
    "print batches of text data (remember, a Field defines how to tokenize/preprocess/label your data and arrange a vocabulary, and does not store the data itself)\n",
    "MISC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0. setting up a default0 Field object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Applying Tokenizer of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Converting word tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(use_vocab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Adding SOS and EOS tokens to input strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(init_token='<SOS>', eos_token='<EOS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Converting text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Fixed Length Seqencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(fix_length=40) # shorter strings will be padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Combining all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(\n",
    "    tokenize=tokenizer,\n",
    "    use_vocab=True,\n",
    "    init_token='<SOS>',\n",
    "    eos_token='<EOS>',\n",
    "    lower=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating a Dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a dataset for language modelling. The input text data will be tokenized and preprocessed according to our Field settings.\n",
    "\n",
    "Ingredients!\n",
    "- a Field object used to store the vocabulary of the text file\n",
    "the path to a text file\n",
    "- an appropriate Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things you can do with a Dataset object\n",
    "- 2.1. print examples from the text\n",
    "Introducing Datasets of various purposes\n",
    "- 2.2. Language modelling (WikiText2)\n",
    "- 2.3. Sentiment analysis (SST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Loading Text file into dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_data = LanguageModelingDataset(path = 'datasets/pg1342.txt',\n",
    "                                  text_field= TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Get examples from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = lm_data.examples\n",
    "print(f\"Number of tokens : {len(examples[0].text)}\")\n",
    "print(f\"First 10 tokens : {examples[0].text[:10]}\")\n",
    "print(f\"Last 10 tokens : {examples[0].text[-10:]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Dataset for language modelling from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_wiki = Field(\n",
    "    tokenize=tokenizer,\n",
    "    use_vocab=True,\n",
    "    init_token='<SOS>',\n",
    "    eos_token='<EOS>',\n",
    "    lower=True,\n",
    "    #fix_length=\n",
    ")\n",
    "\n",
    "# split into train, val, test\n",
    "train, val, test = WikiText2.splits(text_field=TEXT_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Loading Sentiment Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_sst = Field(tokenize=tokenizer, init_token='<SOS>', eos_token='<EOS>',\n",
    "                 lower=True)\n",
    "LABEL_sst = Field(sequential=False)\n",
    "\n",
    "# split into train, val, test\n",
    "train, val, test = datasets.SST.splits(text_field=TEXT_sst, label_field=LABEL_sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using a Vocab object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create a vocabulary of the words from the text file stored in your predefined Field object, TEXT. You first have to build a vocabulary in your Field object using .build_vocab() with your dataset as input. Then you can access it using TEXT.vocab, which is a Vocab object also defined by TorchText. Here is a list of the features provided by Vocab.<br>\n",
    "\n",
    "**Things you can do with a Vocab object**\n",
    "- 3.1. View vocabulary information (size, frequency of words)\n",
    "- 3.2. View the created string2index list (stoi) and index2string dict (itos)\n",
    "- 3.3. Create purpose-specific vocabularies (requires a Counter object)\n",
    "- 3.4. Load external word embeddings\n",
    "- 3.5. Easily handle unknown words</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(lm_data) # use dataset as input\n",
    "vocabulary = TEXT.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Retrieving vocabulary information (size, frequency of words, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size : {len(vocabulary)}\")\n",
    "print(f\"10 most frequent words : {vocabulary.freqs.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Creating 'token to index' and 'index to token' mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 10 words of vocab mapping : {vocabulary.itos[0:10]}\\n\")\n",
    "print(f\"First 10 words of text data: {lm_data.examples[0].text[:10]}\\n\")\n",
    "print(f\"Index of the first word : {vocabulary.stoi[lm_data.examples[0].text[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Create purpose-specific vocabularies (requires a Counter object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = vocabulary.freqs #frequency of the original vocabulary created by Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2 = Vocab(counter=counter,min_freq=10) # discard words appearing less than 10 times\n",
    "vocab3 = Vocab(counter=counter,max_size=100000) # set max number of words for a vocabulary\n",
    "\n",
    "print(len(vocabulary))\n",
    "print(len(vocab2))\n",
    "print(len(vocab3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. load external word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE = Field()\n",
    "lang2 = datasets.LanguageModelingDataset(path='datasets/pg1342.txt',\n",
    "                                       text_field=GLOVE)\n",
    "\n",
    "GLOVE.build_vocab(lang2)\n",
    "\n",
    "# 3.4.2. loading embedding into specific Vocab object\n",
    "vocab2.load_vectors(vectors='glove.6B.50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word embedding size: \", vocab2.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_word = \"humbahumba\"\n",
    "print(\"Index for unknown word %s: %d\" %(unknown_word, vocab2.stoi[unknown_word]))\n",
    "print(\"Token for unknown word: \", vocab2.itos[vocab2.stoi[unknown_word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\n",
    "    'text': ('text',TEXT)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset.splits(\n",
    "    path = '',\n",
    "    train= 'train.json',\n",
    "    format='json',\n",
    "    fields= fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create iterators, we use BucketIterator.splits by specifying the datasets, batch size, and a lambda to tell TorchText what key to use for sorting validation/test sets (traning set is shuffled every epoch).\n",
    "\n",
    "Finally, we can then iterate over batches of the datasets using those iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\n",
    "  'cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterators for train/valid/test datasets\n",
    "train_it, valid_it, test_it = BucketIterator.splits(\n",
    "  (train_data,valid_data, test_data)\n",
    "  sort_key = lambda x: x.text,\n",
    "  sort = True,\n",
    "  batch_size = 32,\n",
    "  device = device\n",
    ")\n",
    "\n",
    "# iterate over training\n",
    "for batch in train_it:\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
